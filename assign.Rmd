---
title: "Practical Machine Learning Assignment"
author: "JIANHAO YE"
date: "2015Äê7ÔÂ24ÈÕ"
output: html_document
---
This data analysis report is for the assignment of the coursera practical machine learning,to train a machine learning model with the data from website below:
http://groupware.les.inf.puc-rio.br/har

¢ÙData Loading
-
```{r}
#download the file of training set
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","train.csv")
#download the file of testing set
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","test.csv")
#read the training set and testing set
training <- read.csv("train.csv")
testing <- read.csv("test.csv")
```

¢ÚData Cleaning
-
sanning both the training and testing set,Since the results are too long, so I will not run the following code:
```{r}
#summary(training)
#summary(testing)
```
Found out some columns have no values, so omit them:
```{r}
#omit the columns that have no values in the testing set and omit the same columns in the training set:
omittedColumns <- c()
for(i in 1:length(testing)){
  if(is.na(testing[1,i])){
    omittedColumns <- c(omittedColumns,i)
  }
}
testing <- testing[,-omittedColumns]
training <- training[,-omittedColumns]
```
sanning both the training and testing set again with head(),Since the results are too long, so I will not run the following code:
```{r}
#head(training)
#head(testing)
```
Found out some columns are of no value to classification(e.g. personal information,index parameter,time related parameter), so omit them:
```{r}
training <- training[,-c(1:5)]
testing <- testing[,-c(1:5)]
```

¢ÛChoose the predictors
-
```{r}
#do the pairs analysis to see the corelations between the variables
trainingC <- training
for(i in 1:length(trainingC)){
  if(class(trainingC[,i])=="factor"){
    trainingC[,i] <- as.numeric(trainingC[,i])
  }
}
corelation <- cor(trainingC)
#to find out high-corelated variables
X <- c()
Y <- c()
for(i in 1:dim(corelation)[1]){
  for(j in 1:dim(corelation)[2]){
    if(corelation[i,j]>=0.5 | corelation[i,j]<=-0.5){
      if(i!=j){
         X <- c(X,i)
         Y <- c(Y,j) 
      }
    }
  }
}
#create the data frame of high-corelated variables
coDataFrame <- data.frame(X,Y,namesX=names(trainingC)[X],namesY=names(trainingC)[Y])
#analyse the data frame
coDataFrame
#Deicede to omit those high-corelated variables
Yunique <- unique(Y)
training <- training[,-Yunique]
testing <- testing[,-Yunique]
```
¢ÜChoose the model and do the machine learning
-
Since this is the problem of classfication, so the non-linear model could be applied. to rise the accuracy, a good option is the Random Forest algrithm, in which the bagging is concerned.And subset the training set into two parts(60%,40%),the 40% of the data set is used for validation.
```{r}
#import the package "caret" training the data with the model of Random Forest
library(caret)
set.seed(311)
inTrain <- createDataPartition(y=training$classe,p=0.6,list=FALSE)
validation <- training[-inTrain,]
training <- training[inTrain,]
fit <- train(classe ~.,data = training,method="rf")
valiPrediction <- predict(fit,newdata = validation)
#to check the accuracy with validation
confusionMatrix(valiPrediction,validation$classe)
```

¢ÝPredict the Testing Set
-
```{r}
testPrediction <- predict(fit,newdata = testing)
#Results
testPrediction
```
Summary
-
in this case,the Random Forest algorithm is used,and I omit some variables regarding the basic knowledge.Next,I calculate the correlations between variables, and omit those high-correlated variables and divide the training set into two parts(60% for training and 40% for validation).The training seems working well that the validation accuracy is 99.8%.